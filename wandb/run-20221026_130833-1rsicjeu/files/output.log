BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "return_dict": false,
  "transformers_version": "4.23.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}
Preprocessing train Data:














 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 21340/21714 [00:29<00:00, 723.59it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21714/21714 [00:30<00:00, 720.60it/s]

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3009/3009 [00:04<00:00, 708.46it/s]
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertForMultipleChoice: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForMultipleChoice were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1:
  0%|                                                                                                                                                                            | 0/10857 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "train_mc.py", line 225, in <module>
    main(args)
  File "train_mc.py", line 112, in main
    train_loss, train_acc = train(model, accelerator, train_loader, optimizer, args, scheduler)
  File "train_mc.py", line 27, in train
    loss = loss / args.accumulation_steps
AttributeError: 'Namespace' object has no attribute 'accumulation_steps'
[31m╭─────────────────────────────── [39m[1mTraceback (most recent call last)[31m[22m ────────────────────────────────╮
[31m│[39m /data/ntu/adl2022/ADL22-HW2/[1mtrain_mc.py[22m:[94m225[39m in [92m<module>[39m                                          [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m   222 │   │   resume = [33m"allow"[39m                                                                   [31m│
[31m│[39m   223 │   )                                                                                      [31m│
[31m│[39m   224 │   artifact = wandb.Artifact([33m"model"[39m, [96mtype[39m=[33m"model"[39m)                                       [31m│
[31m│[39m [31m❱ [39m225 │   main(args)                                                                             [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m /data/ntu/adl2022/ADL22-HW2/[1mtrain_mc.py[22m:[94m112[39m in [92mmain[39m                                              [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m   109 │                                                                                          [31m│
[31m│[39m   110 │   [94mfor[39m epoch [95min[39m [96mrange[39m(start_epoch, args.num_epoch + [94m1[39m):                                   [31m│
[31m│[39m   111 │   │   [96mprint[39m([33mf"Epoch {[39mepoch[33m}:"[39m)                                                           [31m│
[31m│[39m [31m❱ [39m112 │   │   train_loss, train_acc = train(model, accelerator, train_loader, optimizer, args,   [31m│
[31m│[39m   113 │   │   valid_loss, valid_acc = validate(model, valid_loader)                              [31m│
[31m│[39m   114 │   │   [96mprint[39m([33mf"Train Accuracy: {[39mtrain_acc[33m:.2f}, Train Loss: {[39mtrain_loss[33m:.2f}"[39m)            [31m│
[31m│[39m   115 │   │   [96mprint[39m([33mf"Valid Accuracy: {[39mvalid_acc[33m:.2f}, Valid Loss: {[39mvalid_loss[33m:.2f}"[39m)            [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m /data/ntu/adl2022/ADL22-HW2/[1mtrain_mc.py[22m:[94m27[39m in [92mtrain[39m                                              [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m    24 │   │   ids, input_ids, token_type_ids, attention_masks, labels = batch                    [31m│
[31m│[39m    25 │   │   loss, logits = model(input_ids=input_ids, token_type_ids=token_type_ids, attenti   [31m│
[31m│[39m    26 │   │   acc = (logits.argmax(dim=-[94m1[39m) == labels).cpu().float().mean()                       [31m│
[31m│[39m [31m❱ [39m 27 │   │   loss = loss / args.accumulation_steps                                              [31m│
[31m│[39m    28 │   │   accelerator.backward(loss)                                                         [31m│
[31m│[39m    29 │   │                                                                                      [31m│
[31m│[39m    30 │   │   [94mif[39m ((idx + [94m1[39m) % args.accumulation_steps == [94m0[39m) [95mor[39m (idx == [96mlen[39m(data_loader) - [94m1[39m):    [31m│
[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
[1mAttributeError: [32m[22m'Namespace'[39m object has no attribute [32m'accumulation_steps'